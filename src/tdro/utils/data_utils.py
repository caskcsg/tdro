import os
import json
import numpy as np
from tqdm import tqdm
from pathlib import Path
from typing import List, Tuple, Dict, Optional, Union

import torch.distributed as dist

import datasets
from datasets import (
    load_dataset, 
    Value,
    IterableDataset, 
    interleave_datasets, 
    concatenate_datasets
)
from transformers import AutoTokenizer

from .homogenous_iterable_dataset import interleave_datasets_homologenous

import logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Tokenizer
def load_tokenizer(model_name_or_path: str = None, tokenizer_name: str = None, cache_dir: str = None, use_fast: bool = True):
    if tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, cache_dir=cache_dir, use_fast=use_fast)
    elif model_name_or_path:
        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir, use_fast=use_fast)
    else:
        raise ValueError(
            "You are instantiating a new tokenizer from scratch. This is not supported by this script."
            "You can do it from another script, save it, and load it from here, using --tokenizer_name."
        )
    
    # Compatiable with GPT Tokenizers
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})   # Always add a pad token
    tokenizer.padding_side = "right"
    if hasattr(tokenizer, "add_eos_token") and tokenizer.add_eos_token == False:
        # Add </eos> for compatiable with last token pooling
        # Note: This is working for Pythia, LLaMA2 & Mistral, but not LLaMA3, Phi. We need to modify the 
        # `post_processor` of tokenizer.json for them to add </eos>
        tokenizer.add_eos_token = True
    
    return tokenizer

def wc_count(file_name):
    import subprocess
    out = subprocess.getoutput("wc -l %s" % file_name)
    return int(out.split()[0])

def build_corpus_idx_to_row(dataset: datasets.Dataset):
    """ Build a dict on memory of corpus id -> row id of hfdataset """
    idx_to_corpus = dict()
    for row_id, corpus_id in enumerate(dataset["_id"]):
        idx_to_corpus[corpus_id] = row_id
    return idx_to_corpus

def read_corpus(corpus_name_or_path: str):
    """ Load HFDataset from local or online """
    corpus_path = None
    # Local file or folders
    if os.path.exists(corpus_name_or_path):
        if os.path.isdir(corpus_name_or_path):
            files = os.listdir(corpus_name_or_path)
            corpus_path = [
                os.path.join(corpus_name_or_path, f)
                for f in files
                if f.endswith('json') or f.endswith('jsonl')
            ]
        else:
            corpus_path = [corpus_name_or_path]
        
        if corpus_path:
            # Load as Json files
            dataset_name = 'json'
            dataset_split = 'train'
            dataset_language = 'default'
        else:
            # Try to load from local HF dataset
            dataset_name = corpus_name_or_path
            dataset_split = 'train'
            dataset_language = None
            corpus_path = None
    # Online huggingface dataset
    else:
        info = corpus_name_or_path.split('/')
        dataset_split = info[-1] if len(info) == 3 else 'train'
        dataset_name = "/".join(info[:-1]) if len(info) == 3 else '/'.join(info)
        dataset_language = 'default'
        if ':' in dataset_name:
            dataset_name, dataset_language = dataset_name.split(':')
    
    dataset = load_dataset(
        dataset_name,
        dataset_language,
        data_files=corpus_path,
        split=dataset_split
    )

    # Parse tevatron format Jsonl text column names to sentence transformers format
    for _original_column_name, _new_column_name in [("query_id", "_id"), ("docid", "_id"), ("id", "_id"), ("query", "text"), ("question", "text")]:
        if _original_column_name in dataset.column_names:
            dataset = dataset.rename_column(_original_column_name, _new_column_name)
    
    # Format "_id" to str
    if "_id" in dataset.column_names and dataset.features["_id"].dtype != 'string':
        dataset = dataset.cast_column('_id', Value("string"))
    
    return dataset

def process_tsv_file(tsv_ranks_path: str, depth: int=1000):
    q_p_dict = {}
    ret = []    # (qid, pid) pairs
    print(f"Reading idx from {tsv_ranks_path}")
    for _, line in enumerate(tqdm(open(tsv_ranks_path, 'r'), total=wc_count(tsv_ranks_path))):
        line: list = line.split('\t')
        if len(line) == 4:
            line.pop(1)
        qid: str = line[0].strip()
        pid: str = line[1].strip()
        score: str = float(line[2])     # This score is generated by dual-encoder
        if qid not in q_p_dict:
            q_p_dict[qid] = [(pid, score)]
        else:
            q_p_dict[qid].append((pid, score))
    for k, v in q_p_dict.items():
        q_p_dict[k] = sorted(v, key=lambda x: x[1], reverse=True)
        q_p_dict[k] = q_p_dict[k][:depth]
        ret.extend([(k, pid) for pid, score in q_p_dict[k]]) 
    return ret

def load_domain_datasets(
        domain_names: List[str],
        preprocessed_dir: Union[str, Path],
        add_domain_id: bool = False,
        add_domain_name: bool = False,
        domain_to_idx: Optional[Dict[str, int]] = None,
        category_list: Dict[str, List[str]] = None,
    ):
    """
    Load multiple HF dataset by `domain_names` from root dir `preprocessed_dir`
     - `domain_names`: List of each domain names.
     - `preprocessed_dir`: Root folder path of all processed domains.
     - `add_domain_id`: Add domain index.
     - `add_domain_name`: Add domain name.
     - `domain_to_idx`: A dict that map the domain names to coresponding index
    """
    preprocessed_dir = Path(preprocessed_dir)
    domain_files = [preprocessed_dir / (_name + ".jsonl") for _name in sorted(domain_names)]
    
    domain_ds: Dict[str, datasets.Dataset] = dict()
    for domain_idx, domain_filepath in enumerate(domain_files):
        if not (domain_filepath.exists() and domain_filepath.is_file()):
            raise FileNotFoundError(f"{domain_filepath} does not exists or is not a file. Please check data config.")

        if dist.get_rank() in [-1, 0]:
            logger.info(f"Loading [{domain_idx+1}/{len(domain_files)}] domain {domain_filepath.name} from {str(domain_filepath)}")
        
        domain_name: str = domain_filepath.stem
        dataset = load_dataset("json", data_files=str(domain_filepath), split="train")
        
        # Add `_train_dataset_idx` to support reproducable negative passage sampling
        dataset = dataset.add_column(name="_train_dataset_idx", column=np.arange(len(dataset)))

        if add_domain_name:
            dataset = dataset.add_column(name="domain_name", column=[domain_name for _ in range(len(dataset))])
        
        domain_ds[domain_name] = dataset
    
    if category_list is not None:
        if dist.get_rank() in [-1, 0]:
            logger.info(f"Grouping datasets by category..")
        
        category_ds: Dict[str, datasets.Dataset] = dict()
        for category_name, domain_list in category_list.items():
            category_ds[category_name] = concatenate_datasets([domain_ds[i] for i in domain_list])
        
        if add_domain_id:
            for domain_name in category_ds.keys():
                category_ds[domain_name] = category_ds[domain_name].add_column(
                    name="domain_ids", 
                    column=np.ones(len(category_ds[domain_name]), dtype=np.int8) * domain_to_idx[domain_name]
                )
        return category_ds
    else:
        if add_domain_id:
            for domain_name in domain_ds.keys():
                domain_ds[domain_name] = domain_ds[domain_name].add_column(
                    name="domain_ids", 
                    column=np.ones(len(domain_ds[domain_name]), dtype=np.int8) * domain_to_idx[domain_name]
                )
        return domain_ds

def _average_weights(weights):
    if isinstance(weights, list):
        _sum = sum(weights)
        return [i/_sum for i in weights]
    elif isinstance(weights, dict):
        _sum = sum(weights.values())
        return {k: v/_sum for k, v in weights.items()}
    elif isinstance(weights, np.ndarray):
        _sum = sum(weights)
        return weights / _sum
    else:
        raise NotImplementedError()

def construct_domain_dataset(
        domain_config_path: str,
        preprocessed_dir: Union[str, Path],
        add_domain_id: bool = False,
        add_domain_name: bool = False,
        seed: int = 42,
        stopping_strategy: str = 'all_exhausted',
        iterable_n_shards: int = 1024,
        shuffle: bool = False,
        # Homogenous batch sampling
        homogenous_batch: bool = False,
        global_batch_size: Optional[int] = None,
    ):
    """
    Construct interleavable datasets.
     - `domain_config_path`: Path to json format domain config. 
                             1) domain_ids: A dict that map domain name to index.
                             2) domain_weights: Sampling propobality of each train domain.
     - `preprocessed_dir`: Root folder path of all processed domains.
     - `add_domain_id`: Add domain index.
     - `seed`: Random seed for sampling.
     - `stopping_strategy`: Set to 'first_exhausted' for less sampling or 'all_exhausted' for oversampling.
                            See `datasets.interleave_datasets`
     - `iterable_n_shards`: Defines the shards for each domain datasets when converted to iterable dataset.
     - `shuffle`: Shuffle each domain dataset with `seed`.
    """
    # Load domain weights from local file
    with open(domain_config_path, 'r') as f:
        domain_config: dict = json.load(f)
        domain_to_idx: dict = domain_config['domain_ids']
        train_domain_weights_dict: dict = _average_weights(domain_config['domain_weights'])
        category_list: Dict[str, List[str]] = domain_config.get("category_list", None)

    # whenever we convert dict to array, we sort by key
    domain_list = list(sorted(train_domain_weights_dict.keys()))
    num_domains = len(domain_list)

    # Loading datasets of each domains
    domain_names = domain_list if category_list is None else sum(category_list.values(), [])
    domain_ds = load_domain_datasets(
                    domain_names=domain_names,
                    preprocessed_dir=preprocessed_dir,
                    add_domain_id=add_domain_id,
                    add_domain_name=add_domain_name,
                    domain_to_idx=domain_to_idx,
                    category_list=category_list,
                )
    
    if dist.get_rank() in [-1, 0]:
        logger.info(f"{len(domain_ds)} domains loaded. Lengths of each domain:")
        logger.info(f"Domain\t Lengths\t Sampling ratio")
        for _domain_name, _domain_dataset in domain_ds.items():
            logger.info(f"{_domain_name}:\t {len(_domain_dataset)}\t {train_domain_weights_dict[_domain_name]}")
    
    # Convert to IterableDataset and shuffle
    for _domain_name, _domain_dataset in domain_ds.items():
        if dist.get_rank() in [-1, 0]:
            logger.info(f"Convert {_domain_name} to IterableDataset with {iterable_n_shards} shards.")
        domain_ds[_domain_name] = _domain_dataset.to_iterable_dataset(iterable_n_shards)
        if shuffle:
            domain_ds[_domain_name] = domain_ds[_domain_name].shuffle(seed=seed, buffer_size=1000)
    
    if homogenous_batch:
        logger.info(f"Using homogenous batch sampling with global batch size {global_batch_size}")
        full_dataset: IterableDataset = interleave_datasets_homologenous(
                    datasets=[domain_ds[_domain] for _domain in domain_list],
                    batch_size=global_batch_size,
                    probabilities=[train_domain_weights_dict[_domain] for _domain in domain_list],
                    seed=seed,
                    stopping_strategy=stopping_strategy,
                )
    else:
        full_dataset: IterableDataset = interleave_datasets(
                        datasets=[domain_ds[_domain] for _domain in domain_list],
                        probabilities=[train_domain_weights_dict[_domain] for _domain in domain_list],
                        seed=seed,
                        stopping_strategy=stopping_strategy,
                    )
    if dist.get_rank() in [-1, 0]:
        logger.info(f"Finish construct `interleave_datasets`.")
    
    return full_dataset, domain_config
